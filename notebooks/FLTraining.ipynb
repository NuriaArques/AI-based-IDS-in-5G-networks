{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:44.957534Z",
     "iopub.status.busy": "2025-06-14T08:05:44.957054Z",
     "iopub.status.idle": "2025-06-14T08:05:44.964163Z",
     "shell.execute_reply": "2025-06-14T08:05:44.963362Z",
     "shell.execute_reply.started": "2025-06-14T08:05:44.957497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:44.991467Z",
     "iopub.status.busy": "2025-06-14T08:05:44.991256Z",
     "iopub.status.idle": "2025-06-14T08:05:48.513115Z",
     "shell.execute_reply": "2025-06-14T08:05:48.511961Z",
     "shell.execute_reply.started": "2025-06-14T08:05:44.991450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\"\n",
    "!pip install -q opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:48.514835Z",
     "iopub.status.busy": "2025-06-14T08:05:48.514552Z",
     "iopub.status.idle": "2025-06-14T08:05:48.994985Z",
     "shell.execute_reply": "2025-06-14T08:05:48.994070Z",
     "shell.execute_reply.started": "2025-06-14T08:05:48.514801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union, Type\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from copy import deepcopy\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:48.997305Z",
     "iopub.status.busy": "2025-06-14T08:05:48.996665Z",
     "iopub.status.idle": "2025-06-14T08:05:49.013884Z",
     "shell.execute_reply": "2025-06-14T08:05:49.012972Z",
     "shell.execute_reply.started": "2025-06-14T08:05:48.997281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    # DDoS\n",
    "    'DDoS-ACK_Fragmentation': 'DDoS',\n",
    "    'DDoS-UDP_Flood': 'DDoS',\n",
    "    'DDoS-SlowLoris': 'DDoS',\n",
    "    'DDoS-ICMP_Flood': 'DDoS',\n",
    "    'DDoS-RSTFINFlood': 'DDoS',\n",
    "    'DDoS-PSHACK_Flood': 'DDoS',\n",
    "    'DDoS-HTTP_Flood': 'DDoS',\n",
    "    'DDoS-UDP_Fragmentation': 'DDoS',\n",
    "    'DDoS-TCP_Flood': 'DDoS',\n",
    "    'DDoS-SYN_Flood': 'DDoS',\n",
    "    'DDoS-SynonymousIP_Flood': 'DDoS',\n",
    "    'DDoS-ICMP_Fragmentation': 'DDoS',\n",
    "    \n",
    "    # DoS\n",
    "    'DoS-TCP_Flood': 'DoS',\n",
    "    'DoS-HTTP_Flood': 'DoS',\n",
    "    'DoS-SYN_Flood': 'DoS',\n",
    "    'DoS-UDP_Flood': 'DoS',\n",
    "    \n",
    "    # Brute Force\n",
    "    'DictionaryBruteForce': 'Brute Force',\n",
    "    \n",
    "    # Spoofing\n",
    "    'MITM-ArpSpoofing': 'Spoofing',\n",
    "    'DNS_Spoofing': 'Spoofing',\n",
    "    \n",
    "    # Recon\n",
    "    'Recon-PingSweep': 'Recon',\n",
    "    'Recon-OSScan': 'Recon',\n",
    "    'VulnerabilityScan': 'Recon',\n",
    "    'Recon-PortScan': 'Recon',\n",
    "    'Recon-HostDiscovery': 'Recon',\n",
    "    \n",
    "    # Web-based\n",
    "    'SqlInjection': 'Web-based',\n",
    "    'CommandInjection': 'Web-based',\n",
    "    'Backdoor_Malware': 'Web-based',\n",
    "    'Uploading_Attack': 'Web-based',\n",
    "    'XSS': 'Web-based',\n",
    "    'BrowserHijacking': 'Web-based',\n",
    "    \n",
    "    # Mirai\n",
    "    'Mirai-greip_flood': 'Mirai',\n",
    "    'Mirai-greeth_flood': 'Mirai',\n",
    "    'Mirai-udpplain': 'Mirai',\n",
    "    \n",
    "    # Benign Traffic\n",
    "    'BenignTraffic': 'Benign'\n",
    "}\n",
    "\n",
    "def GroupAttacks(label):\n",
    "    return label_mapping.get(label, 'Unknown') # Default to 'unknown'\n",
    "\n",
    "df_labels = pd.read_csv(\"..\\\\datasets\\\\df_labels.csv\")\n",
    "\n",
    "def preprocessing(df: pd.DataFrame, labels: pd.DataFrame = df_labels) -> pd.DataFrame:\n",
    "    \"\"\" Initial data preprocessing. \"\"\"\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    df['magnitude'] = df['magnitue']\n",
    "    df = df.drop(['magnitue'], axis=1)\n",
    "\n",
    "    # Organize label columns\n",
    "    #     label           2 categories (binary): 0: BenignTraffic, 1: Attack\n",
    "    #     attack_cat      8 categories: Bening, DDoS, DoS, Recon, Mirai, Web-based, Spoofing, Brute Force\n",
    "    #     attack_type     34 categories: Benign and 33 attacks\n",
    "    df['grouped_label'] = df['label'].map(GroupAttacks)\n",
    "    df.rename(columns={'label': 'attack_type', 'grouped_label': 'attack_cat'}, inplace=True)\n",
    "    df.replace({'attack_cat': {'BenignTraffic': 'Benign'}, 'attack_type': {'BenignTraffic': 'Benign'}}, inplace=True)\n",
    "    df['label'] = df['attack_cat'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "    # Convert categorical labels to numerical labels\n",
    "    attack_cat_map = dict(zip(labels['attack_cat_name'], labels['attack_cat_num']))\n",
    "    attack_type_map = dict(zip(labels['attack_type_name'], labels['attack_type_num']))\n",
    "\n",
    "    df['attack_cat'] = df['attack_cat'].map(attack_cat_map).astype(int)\n",
    "    df['attack_type'] = df['attack_type'].map(attack_type_map).astype(int)\n",
    "\n",
    "    # Feature Selection\n",
    "    less_important_features = ['drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'http', 'dns', 'telnet', 'smtp', 'ssh', 'irc', 'tcp', 'udp', 'dhcp', 'arp', 'icmp', 'ipv', 'llc', 'number']\n",
    "    df = df.drop(columns=less_important_features)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.015574Z",
     "iopub.status.busy": "2025-06-14T08:05:49.015295Z",
     "iopub.status.idle": "2025-06-14T08:05:49.028566Z",
     "shell.execute_reply": "2025-06-14T08:05:49.027935Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.015552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler() # Initialize a global StandardScaler for reuse across training and test sets\n",
    "\n",
    "def ScaleData_train(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Scales training data features using standard normalization (mean=0, std=1). \"\"\"\n",
    "    \n",
    "    df_scaled = scale.fit_transform(df)\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns) \n",
    "\n",
    "def ScaleData_test(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Scales test/validation data using the training data's fitted scaler \"\"\"\n",
    "    \n",
    "    df_scaled = scale.transform(df)\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns) \n",
    "    \n",
    "\n",
    "def Split(df: pd.DataFrame, training: bool) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\" Splits the DataFrame into features and labels, and scales features. \"\"\"\n",
    "    \n",
    "    X = df.drop(columns=['label', 'attack_cat', 'attack_type'])\n",
    "    X_scaled = ScaleData_train(X) if training else ScaleData_test(X)\n",
    "\n",
    "    # Discard 'atack_cat' as the label will not be used in any model\n",
    "    y_label = df['label']\n",
    "    y_attack = df['attack_type']\n",
    "    \n",
    "    return X_scaled, y_label, y_attack\n",
    "\n",
    "\n",
    "def to_tensor(X: Union[pd.DataFrame, torch.Tensor], y: Union[pd.Series, torch.Tensor]) -> TensorDataset:\n",
    "    \"\"\" Converts input features and labels to PyTorch TensorDataset. \"\"\"\n",
    "    \n",
    "    # Convert features to tensor if needed\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    # Convert labels to tensor if needed\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    return TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.029556Z",
     "iopub.status.busy": "2025-06-14T08:05:49.029354Z",
     "iopub.status.idle": "2025-06-14T08:05:49.052321Z",
     "shell.execute_reply": "2025-06-14T08:05:49.051725Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.029539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3  # Number of federated clients\n",
    "\n",
    "def LoadingPartitions(PARTITIONS: str) -> Dict[int, Tuple[Tuple[Any, Any, Any], Tuple[Any, Any, Any]]]:\n",
    "    \"\"\" Loads and preprocesses federated data partitions, then splits them into train/validation sets per client, returning processed inputs and labels. \"\"\"\n",
    "    \n",
    "    partitions, train_sets, val_sets, node_data = {}, {}, {}, {}\n",
    "    \n",
    "    for i in range(NUM_CLIENTS):\n",
    "        # Load and preprocess the client's partition\n",
    "        partitions[i] = preprocessing(pd.read_csv(f\"..\\\\datasets\\\\{PARTITIONS}\\\\{(PARTITIONS.replace('/', '_').lower())}_part{i}.csv\"))\n",
    "        \n",
    "        # Split into Train (70%) and Validation (30%) \n",
    "        train_sets[i], val_sets[i] = train_test_split(partitions[i], test_size = 0.3, shuffle=True, random_state=42)\n",
    "\n",
    "        # Apply custom Split function to get (X, y_bin, y_multi) and store it\n",
    "        node_data[i] = Split(train_sets[i], True), Split(val_sets[i], False)\n",
    "\n",
    "    return node_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.053420Z",
     "iopub.status.busy": "2025-06-14T08:05:49.053199Z",
     "iopub.status.idle": "2025-06-14T08:05:49.075621Z",
     "shell.execute_reply": "2025-06-14T08:05:49.074859Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.053403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_loaders(batch: int, MODE: str, node_data: Dict[int, Tuple[Tuple[Any, Any, Any], Tuple[Any, Any, Any]]]) -> Tuple[Dict[int, DataLoader], Dict[int, DataLoader]]:\n",
    "    \"\"\" Creates training and validation PyTorch DataLoaders for each federated client, based on the task mode. \"\"\"\n",
    "    \n",
    "    train_loaders, val_loaders = {}, {}\n",
    "\n",
    "    for client in range(NUM_CLIENTS):\n",
    "        (X_train, y_train_bin, y_train_multi), (X_val, y_val_bin, y_val_multi) = node_data[client]\n",
    "\n",
    "        if MODE == 'AD':\n",
    "            data_train = X_train\n",
    "            targets_train = y_train_bin\n",
    "            data_val = X_val\n",
    "            targets_val = y_val_bin\n",
    "        \n",
    "        else: # MODE == 'AC'\n",
    "            # Convert to NumPy arrays to avoid indexing issues\n",
    "            X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "            y_train_multi_np = y_train_multi.values if hasattr(y_train_multi, 'values') else y_train_multi\n",
    "            X_val_np = X_val.values if hasattr(X_val, 'values') else X_val\n",
    "            y_val_multi_np = y_val_multi.values if hasattr(y_val_multi, 'values') else y_val_multi\n",
    "\n",
    "            # Remove benign samples (target == 0) and shift attack labels from 1–33 to 0–32\n",
    "            mask_train = y_train_multi_np != 0\n",
    "            data_train = X_train_np[mask_train]\n",
    "            targets_train = y_train_multi_np[mask_train] - 1 \n",
    "\n",
    "            mask_val = y_val_multi_np != 0\n",
    "            data_val = X_val_np[mask_val]\n",
    "            targets_val = y_val_multi_np[mask_val] - 1\n",
    "            \n",
    "        train_loaders[client] = DataLoader(to_tensor(data_train, targets_train), batch_size=batch, shuffle=True)\n",
    "        val_loaders[client] = DataLoader(to_tensor(data_val, targets_val), batch_size=batch, shuffle=False)\n",
    "\n",
    "    return train_loaders, val_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.076724Z",
     "iopub.status.busy": "2025-06-14T08:05:49.076458Z",
     "iopub.status.idle": "2025-06-14T08:05:49.097808Z",
     "shell.execute_reply": "2025-06-14T08:05:49.096954Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.076699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_dim = 25\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, hidden_units, activation, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(activation())  # Activation function\n",
    "            in_features = hidden_units\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, fc_units, dropout, num_conv_layers, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1 if i == 0 else num_filters, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "            for i in range(num_conv_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * input_dim, fc_units)\n",
    "        self.output = nn.Linear(fc_units, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(torch.relu(self.fc(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, lstm_hidden, num_layers, fc_units, dropout, num_conv_layers, output_dim):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1 if i == 0 else num_filters, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "            for i in range(num_conv_layers)\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(input_size=num_filters, hidden_size=lstm_hidden, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden, fc_units)\n",
    "        self.output = nn.Linear(fc_units, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "        x = x.permute(0, 2, 1)  # Reshape for LSTM (batch, seq_len, features)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(torch.relu(self.fc(x[:, -1, :])))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FEDERATED LEARNING TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AGGREGATION FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.100400Z",
     "iopub.status.busy": "2025-06-14T08:05:49.100189Z",
     "iopub.status.idle": "2025-06-14T08:05:49.117815Z",
     "shell.execute_reply": "2025-06-14T08:05:49.116947Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.100382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def aggregate_models(model: nn.Module, state_dicts: List[Dict[str, torch.Tensor]], aggregation: str, mu: float = 0.0) -> nn.Module:\n",
    "    \"\"\" Aggregates a list of models using the specified aggregation function. \"\"\"\n",
    "\n",
    "    if aggregation == \"GeomMedian\":\n",
    "        # Geometric Median aggregation\n",
    "        return GeomMedian(model, state_dicts)\n",
    "    \n",
    "    elif aggregation == \"FedAvg\":\n",
    "        # Federated Averaging aggregation: simple averaging of model parameters\n",
    "        return FedAvg(model, state_dicts)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation type: {aggregation}\")\n",
    "      \n",
    "    \n",
    "# Geometric Median Aggregation\n",
    "def GeomMedian(model: nn.Module, state_dicts: List[Dict[str, torch.Tensor]]) -> nn.Module:\n",
    "    \"\"\" Geometric Median aggregation \"\"\"\n",
    "    \n",
    "    keys = state_dicts[0].keys()\n",
    "    \n",
    "    # Initialize geometric median with the first model\n",
    "    median_state_dict = {key: state_dicts[0][key] for key in keys}\n",
    "    \n",
    "    # Geometric Median calculation\n",
    "    for key in keys:\n",
    "        tensors = torch.stack([state_dict[key].float() for state_dict in state_dicts], dim=0)\n",
    "        median_state_dict[key] = tensors.median(dim=0)[0] # geometric_median(tensors)\n",
    "    \n",
    "    # Create a new model with the aggregated weights\n",
    "    aggregated_model = model\n",
    "    aggregated_model.load_state_dict(median_state_dict)\n",
    "    return aggregated_model\n",
    "\n",
    "\n",
    "# FedAvg Aggregation\n",
    "def FedAvg(model: nn.Module, state_dicts: List[Dict[str, torch.Tensor]]) -> nn.Module:\n",
    "    \"\"\" FedAvg Aggregation \"\"\"\n",
    "    new_state_dict = state_dicts[0].copy()\n",
    "\n",
    "    for key in new_state_dict.keys():\n",
    "        new_state_dict[key] = torch.stack([state_dict[key].float() for state_dict in state_dicts], dim=0).mean(dim=0)\n",
    "    \n",
    "    # Create a new model with the aggregated weights\n",
    "    aggregated_model = model \n",
    "    aggregated_model.load_state_dict(new_state_dict)\n",
    "    return aggregated_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LOCAL TRAINING AND VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.119052Z",
     "iopub.status.busy": "2025-06-14T08:05:49.118863Z",
     "iopub.status.idle": "2025-06-14T08:05:49.135435Z",
     "shell.execute_reply": "2025-06-14T08:05:49.134855Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.119031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_local_model(model: nn.Module, global_model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, optimizer: torch.optim.Optimizer, criterion: nn.Module, epochs: int, DP: Optional[Dict[str, float]] = {}, snr: bool = False, mu: float = 0.0) -> Tuple[\n",
    "        Dict[str, torch.Tensor],  # Trained model weights\n",
    "        List[float],              # Training accuracies per epoch\n",
    "        List[float],              # Validation accuracies per epoch\n",
    "        List[float],              # Balanced validation accuracies per epoch\n",
    "        List[float],              # Training losses per epoch\n",
    "        List[float]               # Privacy epsilon values per epoch (if DP)\n",
    "    ]:\n",
    "    \"\"\" Trains a local model for one federated client, optionally using differential privacy (DP) and tracking SNR for DP. \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    global_model.to(device)\n",
    "\n",
    "    train_losses, train_accuracies, val_accuracies, val_balanced_accuracies, DP_epsilon = [], [], [], [], []\n",
    "    global_weights = {\n",
    "        name.replace('_module.', ''): param.clone().detach()\n",
    "        for name, param in global_model.named_parameters()\n",
    "    }\n",
    "\n",
    "    if DP:\n",
    "        privacy_engine = PrivacyEngine()\n",
    "        model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "            module = model,\n",
    "            optimizer = optimizer,\n",
    "            data_loader = train_loader,\n",
    "            target_epsilon = DP['EPSILON'],\n",
    "            target_delta = DP['DELTA'],\n",
    "            epochs = epochs,\n",
    "            max_grad_norm = DP['MAX_GRAD_NORM'],\n",
    "            track_clipping=True\n",
    "        )\n",
    "\n",
    "    if snr:\n",
    "        snr_per_epoch, log_snr_per_epoch = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct_train, total_train, epoch_loss = 0, 0, 0\n",
    "        if snr:\n",
    "            epoch_snr_values = [] # Only needed if SNR tracking is on\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            if mu != 0:\n",
    "                fedprox_term = sum(torch.norm(param - global_weights[name.replace('_module.', '')]) ** 2 \n",
    "                                   for name, param in model.named_parameters())\n",
    "                loss += (mu / 2) * fedprox_term\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            if DP and snr:\n",
    "                # Accumulate per-sample gradient norms\n",
    "                grad_sample = []\n",
    "                for p in model.parameters():\n",
    "                    if hasattr(p, \"grad_sample\") and p.grad_sample is not None:\n",
    "                        grad_sample.append(p.grad_sample.view(p.grad_sample.shape[0], -1))  # [batch_size, param_dim]\n",
    "                \n",
    "                if grad_sample:\n",
    "                    stacked = torch.cat(grad_sample, dim=1)  # [batch_size, total_params]\n",
    "                    batch_mean_grad = stacked.mean(dim=0)    # shape: [total_params]\n",
    "                    grad_norm_squared = batch_mean_grad.norm(p=2).item() ** 2  # ||ḡ||^2\n",
    "                    total_params = stacked.shape[1]          # d\n",
    "                    sigma = DP['MAX_GRAD_NORM'] * optimizer.noise_multiplier  # σ = C × noise multiplier\n",
    "                    snr_batch = grad_norm_squared / (total_params * sigma ** 2)\n",
    "                    epoch_snr_values.append(snr_batch)\n",
    "   \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "            total_train += y_batch.size(0)\n",
    "        \n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        train_accuracies.append(correct_train / total_train)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct_val, total_val = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            all_preds, all_labels = [], []\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == y_batch).sum().item()\n",
    "                total_val += y_batch.size(0)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_accuracies.append(correct_val / total_val)\n",
    "        val_balanced_accuracies.append(balanced_accuracy_score(all_labels, all_preds))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_losses[-1]:.4f} - Train Acc: {train_accuracies[-1]:.4f} - Val Acc: {val_accuracies[-1]:.4f}  - Bal Val Acc: {val_balanced_accuracies[-1]:.4f}\")\n",
    "        \n",
    "        if DP:\n",
    "            eps = privacy_engine.accountant.get_epsilon(DP['DELTA'])\n",
    "            DP_epsilon.append(eps)\n",
    "            print(f\"         ε = {eps:.2f}, noise: {optimizer.noise_multiplier}\")\n",
    "            \n",
    "            if snr:\n",
    "                avg_epoch_snr = sum(epoch_snr_values) / len(epoch_snr_values)\n",
    "                snr_per_epoch.append(avg_epoch_snr)\n",
    "\n",
    "                log_snr = math.log10(avg_epoch_snr + 1e-10)  # add epsilon to avoid log(0)\n",
    "                log_snr_per_epoch.append(log_snr)\n",
    "                \n",
    "                print(f\"         Avg SNR = {avg_epoch_snr:.4f}\")\n",
    "                print(f\"         log10(SNR) = {log_snr:.4f}\")\n",
    "\n",
    "    clean_state_dict = { k.replace('_module.', ''): v for k, v in model.state_dict().items() }\n",
    "    \n",
    "    if DP:\n",
    "        epsilon = privacy_engine.get_epsilon(delta = DP['DELTA'])\n",
    "        print(f\" ------- After the {epochs} local epochs: ε = {epsilon:.2f}\")\n",
    "        if snr:\n",
    "            print(f\"    SNR: {snr_per_epoch} ({log_snr_per_epoch})\")\n",
    "    \n",
    "    return clean_state_dict, train_accuracies, val_accuracies, val_balanced_accuracies, train_losses, DP_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.136550Z",
     "iopub.status.busy": "2025-06-14T08:05:49.136287Z",
     "iopub.status.idle": "2025-06-14T08:05:49.159596Z",
     "shell.execute_reply": "2025-06-14T08:05:49.158855Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.136525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def define_optimizer(model: nn.Module, optimizer_name: str, lr: float) -> torch.optim.Optimizer:\n",
    "    \"\"\" Defines and returns an optimizer for the given model based on the optimizer name. \"\"\"\n",
    "    \n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr) \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def define_activation_fn(activation_name: str) -> Type[nn.Module]:\n",
    "    \"\"\" Returns a PyTorch activation function class based on the given name. \"\"\"\n",
    "    \n",
    "    activation_dict = {\n",
    "        \"ReLU\": torch.nn.ReLU,\n",
    "        \"LeakyReLU\": torch.nn.LeakyReLU,\n",
    "        \"Tanh\": torch.nn.Tanh\n",
    "    }\n",
    "    return activation_dict[activation_name]\n",
    "\n",
    "\n",
    "def divide(input_str: str) -> Tuple[str, float]:\n",
    "    \"\"\" Parses a string formatted as 'Name(value)' and extracts the numerical value. \"\"\"\n",
    "    pattern = r\"([A-Za-z]+)\\s?\\(([\\d.]+)\\)\"\n",
    "    match = re.match(pattern, input_str)\n",
    "    return \"FedAvg\", float(match.group(2))\n",
    "\n",
    "\n",
    "def update_dict(metrics: Dict[str, List], model: str, client: Union[str, int], strategy: str, r: int, train_accuracies: List[float], val_accuracies: List[float],\n",
    "                val_balanced_accuracies: List[float], train_losses: List[float], epochs: int, DP_epsilon: List[float]) -> Dict[str, List]:\n",
    "    \"\"\" Updates a metrics dictionary with training and evaluation results for a specific client and round. \"\"\"\n",
    "    \n",
    "    metrics['Model'].extend([model]*epochs)\n",
    "    metrics['Node'].extend([client]*epochs)\n",
    "    metrics['Strategy'].extend([strategy]*epochs)\n",
    "    metrics['Round'].extend([r]*epochs)\n",
    "    metrics['TrainAcc'].extend(train_accuracies)\n",
    "    metrics['ValAcc'].extend(val_accuracies)\n",
    "    metrics['BalValAcc'].extend(val_balanced_accuracies)\n",
    "    metrics['TrainLoss'].extend(train_losses)\n",
    "    metrics['Epsilon'].extend([(r+1)*eps for eps in DP_epsilon])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FULL PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.160677Z",
     "iopub.status.busy": "2025-06-14T08:05:49.160378Z",
     "iopub.status.idle": "2025-06-14T08:05:49.181975Z",
     "shell.execute_reply": "2025-06-14T08:05:49.181385Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.160656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def FederatedLearning(models: Dict[str, torch.nn.Module], model_best_params: Dict[str, Dict[str, Union[str, float, int]]], agg_functions: List[str], \n",
    "                      node_data: Dict[int, any], MODE: str, PARTITIONS: str, NUM_ROUNDS: int, EPOCHS: int, DP: Optional[Dict[str, float]] = {}) -> None:\n",
    "    \"\"\" Runs the federated learning training loop across multiple models, strategies, and clients. \"\"\"\n",
    "    \n",
    "    for model in model_best_params.keys():\n",
    "        print(\"\\n\\n============================================================================================================================\")\n",
    "\n",
    "        print(f\"\\n============== {model} ==============\")\n",
    "        print(f\"Parameters: {model_best_params[model]}\")\n",
    "        print(f\"Num rounds: {NUM_ROUNDS}\\nEpochs per local model: {EPOCHS}\")\n",
    "        \n",
    "        node_metrics = {'Model': [], 'Node': [], 'Strategy': [], 'Round': [], 'TrainAcc': [], 'ValAcc': [], 'BalValAcc': [], 'TrainLoss': [], 'Epsilon': []}\n",
    "\n",
    "        train_loaders, val_loaders = data_loaders(2**model_best_params[model]['batch_size'], MODE, node_data)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        global_model = deepcopy(models[model].to(device))\n",
    "        global_models = {}\n",
    "        \n",
    "        for strategy in agg_functions:\n",
    "            print(f\"\\n\\n------- {strategy} -------\")\n",
    "            st = strategy\n",
    "            global_models[strategy] = deepcopy(global_model) # Initiate new global model\n",
    "            \n",
    "            for r in range(NUM_ROUNDS):\n",
    "                print(f\"\\nRound {r}...\")\n",
    "                \n",
    "                local_weights = []\n",
    "                local_model = deepcopy(global_models[strategy]) # Overwrite 'local_model' with 'global_model'\n",
    "                local_models = {}\n",
    "                (st, mu) = divide(strategy) if strategy.startswith(\"FedProx\") else (st, 0)\n",
    "                \n",
    "                # Train local models \n",
    "                for client in range(NUM_CLIENTS):\n",
    "                    local_model_weights = ' '\n",
    "                    local_models[client] = deepcopy(local_model) # Identical copy for each node\n",
    "\n",
    "                    local_model_weights, train_accuracies, val_accuracies, val_balanced_accuracies, train_losses, DP_epsilon = train_local_model(local_models[client], global_models[strategy], train_loaders[client], val_loaders[client], define_optimizer(local_models[client], model_best_params[model]['optimizer'], model_best_params[model]['learning_rate']), nn.CrossEntropyLoss(), EPOCHS, DP, mu)\n",
    "                    node_metrics = update_dict(node_metrics, model, client, strategy, r, train_accuracies, val_accuracies, val_balanced_accuracies, train_losses, EPOCHS, DP_epsilon)\n",
    "                    local_weights.append(deepcopy(local_model_weights)) # Store local model weights\n",
    "    \n",
    "                # Aggregate local weights to update global model            \n",
    "                global_models[strategy] = deepcopy(aggregate_models(models[model], local_weights, st, mu))\n",
    "                # Save global model checkpoint for this round\n",
    "                path = f\"{MODE}\\\\DP\" if DP else MODE\n",
    "                torch.save(global_models[strategy].state_dict(), f\"..\\\\models\\\\{path}\\\\{MODE}{EPOCHS}_{PARTITIONS}_{model}_{strategy}_r{r}.pth\")\n",
    "           \n",
    "        metrics = pd.DataFrame(node_metrics).round(4)\n",
    "        modDP = f\"-DP{int(DP['EPSILON']*NUM_ROUNDS)}-{DP['MAX_GRAD_NORM']}\" if DP else ''\n",
    "        path = f\"{MODE}\\\\DP\" if DP else MODE\n",
    "        metrics.to_csv(f\"..\\\\metrics\\\\{path}\\\\{MODE}{EPOCHS}{modDP}_{PARTITIONS}_Metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ATTACK DETECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:05:49.183169Z",
     "iopub.status.busy": "2025-06-14T08:05:49.182870Z",
     "iopub.status.idle": "2025-06-14T08:05:49.252054Z",
     "shell.execute_reply": "2025-06-14T08:05:49.251517Z",
     "shell.execute_reply.started": "2025-06-14T08:05:49.183140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODE = 'AD' \n",
    "CLASSES = 2 if MODE == 'AD' else 33 # 33 types of attacks\n",
    "NUM_ROUNDS = 10\n",
    "\n",
    "for m in ['models', 'metrics']:\n",
    "    os.makedirs(f\"..\\\\{m}\\\\{MODE}\", exist_ok=True)\n",
    "    os.makedirs(f\"..\\\\{m}\\\\{MODE}\\\\DP\", exist_ok=True)\n",
    "\n",
    "AD_model_best_params = {\n",
    "    'DNN': {'hidden_layers': 3, 'hidden_units': 58, 'activation': 'Tanh', 'learning_rate': 3e-3, 'optimizer': 'Adam', 'batch_size': 9},\n",
    "    'CNN': {'num_filters': 115, 'fc_units': 60, 'dropout': 0.17, 'num_conv_layers': 3, 'learning_rate': 2e-4, 'optimizer': 'Adam', 'batch_size': 7},\n",
    "    'CNN-LSTM': {'num_filters': 114, 'lstm_hidden': 64, 'num_layers': 1, 'fc_units': 49, 'dropout': 0.43, 'num_conv_layers': 3, 'learning_rate': 3e-4, 'optimizer': 'Adam', 'batch_size': 5}\n",
    "}\n",
    "\n",
    "AD_models = {    \n",
    "    'DNN': DNN(input_dim, hidden_layers = AD_model_best_params['DNN']['hidden_layers'], hidden_units = AD_model_best_params['DNN']['hidden_units'], activation = define_activation_fn(AD_model_best_params['DNN']['activation']), output_dim=CLASSES),\n",
    "    'CNN': CNN(input_dim,  AD_model_best_params['CNN']['num_filters'], AD_model_best_params['CNN']['fc_units'],  AD_model_best_params['CNN']['dropout'],  AD_model_best_params['CNN']['num_conv_layers'], output_dim=CLASSES),\n",
    "    'CNN-LSTM': CNN_LSTM(input_dim, AD_model_best_params['CNN-LSTM']['num_filters'], AD_model_best_params['CNN-LSTM']['lstm_hidden'],  AD_model_best_params['CNN-LSTM']['num_layers'], AD_model_best_params['CNN-LSTM']['fc_units'], AD_model_best_params['CNN-LSTM']['dropout'],  AD_model_best_params['CNN-LSTM']['num_conv_layers'], output_dim=CLASSES)\n",
    "}\n",
    "\n",
    "# DP_AD = {'DELTA': 2e-6, 'MAX_GRAD_NORM': 1, 'EPSILON': 2/NUM_ROUNDS}\n",
    "# DP_AD_HT = {'DELTA': 2e-6, 'MAX_GRAD_NORM': 1.97, 'EPSILON': 2.52/NUM_ROUNDS}\n",
    "DP_AD, DP_AD_HT = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for PARTITIONS in ['UNIFORM', 'UNBALANCED']:\n",
    "    node_data = LoadingPartitions(PARTITIONS)\n",
    "    \n",
    "    for EPOCHS in [1, 3, 5, 10]:\n",
    "        agg_functions = [\"GeomMedian\", \"FedAvg\", \"FedProx (1)\", \"FedProx (0.1)\", \"FedProx (0.01)\", \"FedProx (0.001)\"]\n",
    "        print(f\"Using the {PARTITIONS} partitions.\")\n",
    "        if DP_AD:\n",
    "            DP_AD = DP_AD_HT if DP_AD_HT else DP_AD\n",
    "            print(f\"Differential Privacy parameters: {DP_AD}\")\n",
    "        FederatedLearning(AD_models, AD_model_best_params, agg_functions, node_data, MODE, PARTITIONS, NUM_ROUNDS, EPOCHS, DP_AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ATTACK CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T08:06:25.872567Z",
     "iopub.status.idle": "2025-06-14T08:06:25.872836Z",
     "shell.execute_reply": "2025-06-14T08:06:25.872715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODE = 'AC'\n",
    "CLASSES = 2 if MODE == 'AD' else 33 # 33 types of attacks\n",
    "NUM_ROUNDS = 10\n",
    "\n",
    "for m in ['models', 'metrics']:\n",
    "    os.makedirs(f\"..\\\\{m}\\\\{MODE}\", exist_ok=True)\n",
    "    os.makedirs(f\"..\\\\{m}\\\\{MODE}\\\\DP\", exist_ok=True)\n",
    "\n",
    "AC_model_best_params = {\n",
    "    'DNN': {'hidden_layers': 3, 'hidden_units': 100, 'activation': 'Tanh', 'learning_rate': 2.78e-4, 'optimizer': 'Adam', 'batch_size': 6},\n",
    "    'CNN': {'num_filters': 51, 'fc_units': 55, 'dropout': 0.1, 'num_conv_layers': 3, 'learning_rate': 1.4e-3, 'optimizer': 'RMSprop', 'batch_size': 7}, # CNN using 25 input features\n",
    "    'CNN-LSTM': {'num_filters': 71, 'lstm_hidden': 28, 'num_layers': 2, 'fc_units': 61, 'dropout': 0.384, 'num_conv_layers': 1, 'learning_rate': 1.62e-3, 'optimizer': 'RMSprop', 'batch_size': 5},\n",
    "}\n",
    "\n",
    "AC_models = {\n",
    "    'DNN': DNN(input_dim, hidden_layers = AC_model_best_params['DNN']['hidden_layers'], hidden_units = AC_model_best_params['DNN']['hidden_units'], activation = define_activation_fn(AC_model_best_params['DNN']['activation']), output_dim=CLASSES),\n",
    "    'CNN': CNN(input_dim,  AC_model_best_params['CNN']['num_filters'],  AC_model_best_params['CNN']['fc_units'],  AC_model_best_params['CNN']['dropout'],  AC_model_best_params['CNN']['num_conv_layers'], output_dim=CLASSES),\n",
    "    'CNN-LSTM': CNN_LSTM(input_dim, AC_model_best_params['CNN-LSTM']['num_filters'], AC_model_best_params['CNN-LSTM']['lstm_hidden'],  AC_model_best_params['CNN-LSTM']['num_layers'], AC_model_best_params['CNN-LSTM']['fc_units'], AC_model_best_params['CNN-LSTM']['dropout'],  AC_model_best_params['CNN-LSTM']['num_conv_layers'], output_dim=CLASSES)\n",
    "}\n",
    "\n",
    "# DP_AC = {'DELTA': 2e-6, 'MAX_GRAD_NORM': 1, 'EPSILON': 3/NUM_ROUNDS}\n",
    "# DP_AC_HT = {'DELTA': 2e-6, 'MAX_GRAD_NORM': 1.99, 'EPSILON':  2.13/NUM_ROUNDS} # MCC: 0.6952\n",
    "DP_AC, DP_AC_HT = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T08:06:25.873322Z",
     "iopub.status.idle": "2025-06-14T08:06:25.873674Z",
     "shell.execute_reply": "2025-06-14T08:06:25.873540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for PARTITIONS in ['UNIFORM', 'UNBALANCED']:\n",
    "    node_data = LoadingPartitions(PARTITIONS)\n",
    "    \n",
    "    for EPOCHS in [1, 3, 5, 10]:\n",
    "        agg_functions = [\"GeomMedian\", \"FedAvg\", \"FedProx (0.01)\", \"FedProx (0.001)\"]\n",
    "        print(f\"Using the {PARTITIONS} partitions.\")\n",
    "        if DP_AC:\n",
    "            DP_AC = DP_AC_HT if DP_AC_HT else DP_AC\n",
    "            print(f\"Differential Privacy parameters: {DP_AC}\")\n",
    "        FederatedLearning(AC_models, AC_model_best_params, agg_functions, node_data, MODE, PARTITIONS, NUM_ROUNDS, EPOCHS, DP_AC)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6919881,
     "sourceId": 11794545,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
