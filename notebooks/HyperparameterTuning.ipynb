{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c54847",
   "metadata": {
    "papermill": {
     "duration": 0.006699,
     "end_time": "2025-04-04T08:39:03.192566",
     "exception": false,
     "start_time": "2025-04-04T08:39:03.185867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ce92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:03.206688Z",
     "iopub.status.busy": "2025-04-04T08:39:03.206466Z",
     "iopub.status.idle": "2025-04-04T08:39:05.743437Z",
     "shell.execute_reply": "2025-04-04T08:39:05.742549Z"
    },
    "papermill": {
     "duration": 2.545803,
     "end_time": "2025-04-04T08:39:05.745100",
     "exception": false,
     "start_time": "2025-04-04T08:39:03.199297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, matthews_corrcoef\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "\n",
    "import optuna\n",
    "from opacus import PrivacyEngine\n",
    "from plotly.io import show\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef63681",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebda2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:05.760466Z",
     "iopub.status.busy": "2025-04-04T08:39:05.759939Z",
     "iopub.status.idle": "2025-04-04T08:39:05.768679Z",
     "shell.execute_reply": "2025-04-04T08:39:05.767858Z"
    },
    "papermill": {
     "duration": 0.017608,
     "end_time": "2025-04-04T08:39:05.769993",
     "exception": false,
     "start_time": "2025-04-04T08:39:05.752385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    # DDoS\n",
    "    'DDoS-ACK_Fragmentation': 'DDoS',\n",
    "    'DDoS-UDP_Flood': 'DDoS',\n",
    "    'DDoS-SlowLoris': 'DDoS',\n",
    "    'DDoS-ICMP_Flood': 'DDoS',\n",
    "    'DDoS-RSTFINFlood': 'DDoS',\n",
    "    'DDoS-PSHACK_Flood': 'DDoS',\n",
    "    'DDoS-HTTP_Flood': 'DDoS',\n",
    "    'DDoS-UDP_Fragmentation': 'DDoS',\n",
    "    'DDoS-TCP_Flood': 'DDoS',\n",
    "    'DDoS-SYN_Flood': 'DDoS',\n",
    "    'DDoS-SynonymousIP_Flood': 'DDoS',\n",
    "    'DDoS-ICMP_Fragmentation': 'DDoS',\n",
    "    \n",
    "    # DoS\n",
    "    'DoS-TCP_Flood': 'DoS',\n",
    "    'DoS-HTTP_Flood': 'DoS',\n",
    "    'DoS-SYN_Flood': 'DoS',\n",
    "    'DoS-UDP_Flood': 'DoS',\n",
    "    \n",
    "    # Brute Force\n",
    "    'DictionaryBruteForce': 'Brute Force',\n",
    "    \n",
    "    # Spoofing\n",
    "    'MITM-ArpSpoofing': 'Spoofing',\n",
    "    'DNS_Spoofing': 'Spoofing',\n",
    "    \n",
    "    # Recon\n",
    "    'Recon-PingSweep': 'Recon',\n",
    "    'Recon-OSScan': 'Recon',\n",
    "    'VulnerabilityScan': 'Recon',\n",
    "    'Recon-PortScan': 'Recon',\n",
    "    'Recon-HostDiscovery': 'Recon',\n",
    "    \n",
    "    # Web-based\n",
    "    'SqlInjection': 'Web-based',\n",
    "    'CommandInjection': 'Web-based',\n",
    "    'Backdoor_Malware': 'Web-based',\n",
    "    'Uploading_Attack': 'Web-based',\n",
    "    'XSS': 'Web-based',\n",
    "    'BrowserHijacking': 'Web-based',\n",
    "    \n",
    "    # Mirai\n",
    "    'Mirai-greip_flood': 'Mirai',\n",
    "    'Mirai-greeth_flood': 'Mirai',\n",
    "    'Mirai-udpplain': 'Mirai',\n",
    "    \n",
    "    # Benign Traffic\n",
    "    'BenignTraffic': 'Benign'\n",
    "}\n",
    "\n",
    "def GroupAttacks(label):\n",
    "    return label_mapping.get(label, 'Unknown') # Default to 'unknown'\n",
    "\n",
    "df_labels = pd.read_csv(\"..\\\\datasets\\\\df_labels.csv\")\n",
    "\n",
    "def preprocessing(df: pd.DataFrame, labels: pd.DataFrame = df_labels) -> pd.DataFrame:\n",
    "    \"\"\"Initial data preprocessing\"\"\"\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    df['magnitude'] = df['magnitue']\n",
    "    df = df.drop(['magnitue'], axis=1)\n",
    "\n",
    "    # Organize label columns\n",
    "    #     label           2 categories (binary): 0: BenignTraffic, 1: Attack\n",
    "    #     attack_cat      8 categories: Bening, DDoS, DoS, Recon, Mirai, Web-based, Spoofing, Brute Force\n",
    "    #     attack_type     34 categories: Benign and 33 attacks\n",
    "    df['grouped_label'] = df['label'].map(GroupAttacks)\n",
    "    df.rename(columns={'label': 'attack_type', 'grouped_label': 'attack_cat'}, inplace=True)\n",
    "    df.replace({'attack_cat': {'BenignTraffic': 'Benign'}, 'attack_type': {'BenignTraffic': 'Benign'}}, inplace=True)\n",
    "    df['label'] = df['attack_cat'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "    # Convert categorical labels to numerical labels\n",
    "    attack_cat_map = dict(zip(labels['attack_cat_name'], labels['attack_cat_num']))\n",
    "    attack_type_map = dict(zip(labels['attack_type_name'], labels['attack_type_num']))\n",
    "\n",
    "    df['attack_cat'] = df['attack_cat'].map(attack_cat_map).astype(int)\n",
    "    df['attack_type'] = df['attack_type'].map(attack_type_map).astype(int)\n",
    "\n",
    "    # Feature Selection\n",
    "    less_important_features = ['drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'http', 'dns', 'telnet', 'smtp', 'ssh', 'irc', 'tcp', 'udp', 'dhcp', 'arp', 'icmp', 'ipv', 'llc', 'number']\n",
    "    df = df.drop(columns=less_important_features)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778527aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:07.080698Z",
     "iopub.status.busy": "2025-04-04T08:39:07.080430Z",
     "iopub.status.idle": "2025-04-04T08:39:07.089389Z",
     "shell.execute_reply": "2025-04-04T08:39:07.088390Z"
    },
    "papermill": {
     "duration": 0.022723,
     "end_time": "2025-04-04T08:39:07.090735",
     "exception": false,
     "start_time": "2025-04-04T08:39:07.068012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = df_labels.groupby('attack_cat_name')['count'].sum().reset_index().sort_values('count', ascending=False)\n",
    "\n",
    "total = samples['count'].values.sum()\n",
    "benign = samples[samples['attack_cat_name'] == 'Benign']['count'].values[0]\n",
    "attacks = (samples[samples['attack_cat_name'] != 'Benign']['count'].values).sum()\n",
    "\n",
    "print(f\"There are {benign} ({benign/total *100:.2f}%) benign samples and {attacks} ({attacks/total*100:.2f}%) attacks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbc566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:07.144523Z",
     "iopub.status.busy": "2025-04-04T08:39:07.144281Z",
     "iopub.status.idle": "2025-04-04T08:39:07.149188Z",
     "shell.execute_reply": "2025-04-04T08:39:07.148544Z"
    },
    "papermill": {
     "duration": 0.018468,
     "end_time": "2025-04-04T08:39:07.150315",
     "exception": false,
     "start_time": "2025-04-04T08:39:07.131847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler() # Initialize a global StandardScaler for reuse across training and test sets\n",
    "\n",
    "def ScaleData_train(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Scales training data features using standard normalization (mean=0, std=1). \"\"\"\n",
    "    \n",
    "    df_scaled = scale.fit_transform(df)\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns) \n",
    "\n",
    "def ScaleData_test(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Scales test/validation data using the training data's fitted scaler \"\"\"\n",
    "    \n",
    "    df_scaled = scale.transform(df)\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns) \n",
    "    \n",
    "\n",
    "def Split(df: pd.DataFrame, training: bool) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\" Splits the DataFrame into features and labels, and scales features. \"\"\"\n",
    "    \n",
    "    X = df.drop(columns=['label', 'attack_cat', 'attack_type'])\n",
    "    X_scaled = ScaleData_train(X) if training else ScaleData_test(X)\n",
    "\n",
    "    # Discard 'atack_cat' as the label will not be used in any model\n",
    "    y_label = df['label']\n",
    "    y_attack = df['attack_type']\n",
    "    \n",
    "    return X_scaled, y_label, y_attack\n",
    "\n",
    "\n",
    "def to_tensor(X: Union[pd.DataFrame, torch.Tensor], y: Union[pd.Series, torch.Tensor]) -> TensorDataset:\n",
    "    \"\"\" Converts input features and labels to PyTorch TensorDataset. \"\"\"\n",
    "\n",
    "    return TensorDataset(torch.tensor(X.to_numpy(), dtype=torch.float32), torch.tensor(y.to_numpy(), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b52439",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3  # Number of federated clients\n",
    "EPOCHS = 5  # Training epochs for PyTorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6da7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:07.174096Z",
     "iopub.status.busy": "2025-04-04T08:39:07.173891Z",
     "iopub.status.idle": "2025-04-04T08:39:27.625429Z",
     "shell.execute_reply": "2025-04-04T08:39:27.624588Z"
    },
    "papermill": {
     "duration": 20.465622,
     "end_time": "2025-04-04T08:39:27.627213",
     "exception": false,
     "start_time": "2025-04-04T08:39:07.161591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "uniform_partitions = {}\n",
    "uniform_train_sets = {}\n",
    "uniform_val_sets = {}\n",
    "\n",
    "node_data = {}\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    uniform_partitions[i] = preprocessing(pd.read_csv(f\"..\\\\datasets\\\\UniformPartitions\\\\uniform_part{i}.csv\"))\n",
    "    \n",
    "    # Split into Train (70%) and Validation (30%) \n",
    "    uniform_train_sets[i], uniform_val_sets[i] = train_test_split(uniform_partitions[i], test_size = 0.3, shuffle=True, random_state=42)\n",
    "    node_data[i] = Split(uniform_train_sets[i], True), Split(uniform_val_sets[i], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4741442",
   "metadata": {
    "papermill": {
     "duration": 0.01152,
     "end_time": "2025-04-04T08:39:27.651754",
     "exception": false,
     "start_time": "2025-04-04T08:39:27.640234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Structure of the Dictionary `node_data`:\n",
    "\n",
    " * `node_data[i][0]`: returns 3 datasets (`X_train`, `y_train_bin` and `y_train_multi`) of node `i` that will be used for training\n",
    " * `node_data[i][1]`: returns 3 datasets (`X_val`, `y_val_bin` and `y_val_multi`) of node `i` that will be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c91129",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = node_data[0][0][0].shape[1] # 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb25346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_dataset(MODE: str) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\" Constructs a full training and validation dataset from all clients' local data. \"\"\"\n",
    "    \n",
    "    j = 1 if (MODE == 'binary') else 2\n",
    "    \n",
    "    X_train = pd.concat([node_data[i][0][0] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "    y_train = pd.concat([node_data[i][0][j] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "    \n",
    "    X_val = pd.concat([node_data[i][1][0] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "    y_val = pd.concat([node_data[i][1][j] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "\n",
    "    # Only exclude label 0 ('Benign') if MODE is not 'binary'\n",
    "    if MODE != 'binary':\n",
    "        mask_train = y_train != 0\n",
    "        mask_val = y_val != 0\n",
    "\n",
    "        X_train, y_train = X_train[mask_train], y_train[mask_train]\n",
    "        X_val, y_val = X_val[mask_val], y_val[mask_val]\n",
    "\n",
    "        # Ensure labels are zero-indexed (1-33 -> 0-32)\n",
    "        y_train = y_train - 1\n",
    "        y_val = y_val - 1\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a273c5a",
   "metadata": {
    "papermill": {
     "duration": 0.011328,
     "end_time": "2025-04-04T08:39:27.730074",
     "exception": false,
     "start_time": "2025-04-04T08:39:27.718746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DEEP LEARNING (DL) MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fca2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.308617Z",
     "iopub.status.busy": "2025-04-04T08:39:32.308395Z",
     "iopub.status.idle": "2025-04-04T08:39:32.313062Z",
     "shell.execute_reply": "2025-04-04T08:39:32.312435Z"
    },
    "papermill": {
     "duration": 0.017639,
     "end_time": "2025-04-04T08:39:32.314196",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.296557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, hidden_units, activation, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(activation())  # Activation function\n",
    "            in_features = hidden_units\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, fc_units, dropout, num_conv_layers, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1 if i == 0 else num_filters, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "            for i in range(num_conv_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * input_dim, fc_units)\n",
    "        self.output = nn.Linear(fc_units, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(torch.relu(self.fc(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, lstm_hidden, num_layers, fc_units, dropout, num_conv_layers, output_dim):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1 if i == 0 else num_filters, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "            for i in range(num_conv_layers)\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(input_size=num_filters, hidden_size=lstm_hidden, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden, fc_units)\n",
    "        self.output = nn.Linear(fc_units, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "        x = x.permute(0, 2, 1)  # Reshape for LSTM (batch, seq_len, features)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(torch.relu(self.fc(x[:, -1, :])))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b305f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\" Evaluate model predictions using Matthews Correlation Coefficient (MCC). \"\"\"\n",
    "    \n",
    "    # accuracy = accuracy_score(y_true, y_pred)\n",
    "    # precision = precision_score(y_true, y_pred)\n",
    "    # recall = recall_score(y_true, y_pred)\n",
    "    # f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return mcc\n",
    "\n",
    "\n",
    "def train_pytorch(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, optimizer: torch.optim.Optimizer, criterion: nn.Module, epochs: int, DP: Optional[Dict[str, float]] = {}, trial: Optional[object] = None\n",
    "                  ) -> Tuple[Dict[str, torch.Tensor], Union[float, Tuple[float, float]]]:\n",
    "    \"\"\" Trains a local model for one federated client, optionally using differential privacy (DP). \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    if DP:\n",
    "        privacy_engine = PrivacyEngine()\n",
    "        model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "                module = model,\n",
    "                optimizer = optimizer,\n",
    "                data_loader = train_loader,\n",
    "                target_epsilon = DP['EPSILON'],\n",
    "                target_delta = DP['DELTA'],\n",
    "                epochs = epochs,\n",
    "                max_grad_norm = DP['MAX_GRAD_NORM']\n",
    "        )\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted.cpu())\n",
    "            all_labels.append(y_batch.cpu())\n",
    "    y_pred, y_true = torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "    mcc = evaluate(y_true, y_pred)\n",
    "\n",
    "\n",
    "    if DP:\n",
    "        epsilon = privacy_engine.get_epsilon(delta = DP['DELTA'])\n",
    "        if trial:\n",
    "            trial.set_user_attr(\"epsilon\", epsilon)\n",
    "        print(f\"MCC: {mcc:.4f}, ε = {epsilon:.2f}\")\n",
    "    \n",
    "    return model.state_dict(), (mcc, epsilon if DP else mcc)  # Return local trained model weights and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bbc35",
   "metadata": {
    "papermill": {
     "duration": 0.011097,
     "end_time": "2025-04-04T08:39:32.589059",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.577962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **HYPERPARAMETER TUNNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04cbe98",
   "metadata": {},
   "source": [
    "## **STUDIES DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af02f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.612482Z",
     "iopub.status.busy": "2025-04-04T08:39:32.612253Z",
     "iopub.status.idle": "2025-04-04T08:39:32.623249Z",
     "shell.execute_reply": "2025-04-04T08:39:32.622651Z"
    },
    "papermill": {
     "duration": 0.023973,
     "end_time": "2025-04-04T08:39:32.624459",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.600486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "activation_dict = {\n",
    "    \"ReLU\": torch.nn.ReLU,\n",
    "    \"LeakyReLU\": torch.nn.LeakyReLU,\n",
    "    \"Tanh\": torch.nn.Tanh\n",
    "}\n",
    "\n",
    "def define_optimizer(model: nn.Module, optimizer_name: str, lr: float) -> torch.optim.Optimizer:\n",
    "    \"\"\" Defines and returns an optimizer for the given model based on the optimizer name. \"\"\"\n",
    "    \n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr) \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def tune_DNN(trial: optuna.trial.Trial, X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series, MODE: str, CLASSES: int) -> float:\n",
    "    \"\"\" Tune a feedforward DNN using Optuna. \"\"\"\n",
    "\n",
    "    hidden_layers = trial.suggest_int(\"hidden_layers\", 1, 5)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 16, 128)\n",
    "    activation_name = trial.suggest_categorical(\"activation\", list(activation_dict.keys()))\n",
    "    activation_fn = activation_dict[activation_name]\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "\n",
    "    model = DNN(input_dim, hidden_layers, hidden_units, activation_fn, output_dim=CLASSES).to(device)\n",
    "    criterion = nn.CrossEntropyLoss() if MODE == 'binary' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, optimizer_name, lr)\n",
    "    \n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**batch_size, shuffle=False)\n",
    "    \n",
    "    _, mcc = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 10) # Small number of epochs for tuning\n",
    "    return mcc\n",
    "    \n",
    "\n",
    "def tune_CNN(trial: optuna.trial.Trial, X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series, MODE: str, CLASSES: int) -> float:\n",
    "    \"\"\" Tune a CNN using Optuna. \"\"\"\n",
    "\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 32, 128)\n",
    "    fc_units = trial.suggest_int(\"fc_units\", 16, 64)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "    \n",
    "    model = CNN(input_dim, num_filters=num_filters, fc_units=fc_units, dropout=dropout, num_conv_layers=num_conv_layers, output_dim=CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss() if MODE == 'binary' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, optimizer_name, lr)\n",
    "\n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**batch_size, shuffle=False)\n",
    "    \n",
    "    _, mcc = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 10) # Small number of epochs for tuning\n",
    "    return mcc\n",
    "    \n",
    "    \n",
    "def tune_CNN_LSTM(trial: optuna.trial.Trial, X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series, MODE: str, CLASSES: int) -> float:\n",
    "    \"\"\" Tune a CNN-LSTM using Optuna. \"\"\"\n",
    "\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 32, 128)\n",
    "    lstm_hidden = trial.suggest_int(\"lstm_hidden\", 16, 64)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    fc_units = trial.suggest_int(\"fc_units\", 16, 64)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "    \n",
    "    model = CNN_LSTM(input_dim, num_filters=num_filters, lstm_hidden=lstm_hidden, num_layers=num_layers, fc_units=fc_units, dropout=dropout, num_conv_layers=num_conv_layers, output_dim=CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss() if MODE == 'binary' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, optimizer_name, lr)\n",
    "    \n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**batch_size, shuffle=False)\n",
    "    \n",
    "    _, mcc = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 10) # Small number of epochs for tuning\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ce73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_DP(trial, X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series, mode: str) -> Tuple[float, float]:\n",
    "    \"\"\" Tune a CNN incorporating DP using Optuna. \"\"\"\n",
    "\n",
    "    # Predefined best CNN architecture parameters \n",
    "    CNN_best_params = {\n",
    "        'AD': {'num_filters': 115, 'fc_units': 60, 'dropout': 0.17, 'num_conv_layers': 3, 'learning_rate': 2e-4, 'optimizer': 'Adam', 'batch_size': 7},\n",
    "        'AC': {'num_filters': 51, 'fc_units': 55, 'dropout': 0.1, 'num_conv_layers': 3, 'learning_rate': 1.4e-3, 'optimizer': 'RMSprop', 'batch_size': 7}\n",
    "    }\n",
    "\n",
    "    # Instantiate fixed model architecture\n",
    "    CNN_models = {\n",
    "        'AD': CNN(input_dim,  CNN_best_params['AD']['num_filters'], CNN_best_params['AD']['fc_units'],  CNN_best_params['AD']['dropout'],  CNN_best_params['AD']['num_conv_layers'], output_dim=2),\n",
    "        'AC': CNN(input_dim,  CNN_best_params['AC']['num_filters'],  CNN_best_params['AC']['fc_units'],  CNN_best_params['AC']['dropout'],  CNN_best_params['AC']['num_conv_layers'], output_dim=33)\n",
    "    }\n",
    "\n",
    "    # Tuning differential privacy parameters\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 0.5, 10.0)  # lower ε = more private\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.5, 3.0)  # Gradient clipping threshold\n",
    "\n",
    "    model = CNN_models[mode]\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() if mode == 'AD' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, CNN_best_params[mode]['optimizer'], CNN_best_params[mode]['learning_rate'])\n",
    "    \n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**CNN_best_params[mode]['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**CNN_best_params[mode]['batch_size'], shuffle=True)\n",
    "\n",
    "    DP = {\n",
    "        'MAX_GRAD_NORM': max_grad_norm,\n",
    "        'DELTA': 2e-6,\n",
    "        'EPSILON': epsilon / NUM_CLIENTS \n",
    "    }\n",
    "    \n",
    "    # Run training with DP enabled\n",
    "    _, (mcc, eps) = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 2, DP, trial) # Small number of epochs for tuning\n",
    "    return mcc, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e52cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_study(best_params: Dict[str, Dict[str, Any]], feat_importance: Dict[str, Dict[str, float]], best_scores: Dict[str, List[float]],\n",
    "                         studies: Dict[str, optuna.Study], MODE: str, CLASSES: int, DP_mode: str) -> None:\n",
    "\n",
    "    \"\"\" Runs a hyperparameter optimization study for each model (DNN, CNN, CNN-LSTM, and DP-CNN) using Optuna. \"\"\"\n",
    "\n",
    "    def objective(trial, model_name):\n",
    "        return models_dict[model_name](trial)\n",
    "    \n",
    "    # Load full aggregated dataset for tuning\n",
    "    X_train, y_train, X_val, y_val = full_dataset(MODE)\n",
    "\n",
    "    # Define model-specific tuning functions\n",
    "    models_dict = {\n",
    "        'DNN': lambda trial: tune_DNN(trial, X_train, y_train, X_val, y_val, MODE, CLASSES),\n",
    "        'CNN': lambda trial: tune_CNN(trial, X_train, y_train, X_val, y_val, MODE, CLASSES),\n",
    "        'CNN-LSTM': lambda trial: tune_CNN_LSTM(trial, X_train, y_train, X_val, y_val, MODE, CLASSES),\n",
    "        'DP': lambda trial: tune_DP(trial, X_train, y_train, X_val, y_val, DP_mode)\n",
    "    }\n",
    "\n",
    "\n",
    "    for model in models_dict.keys():\n",
    "        \n",
    "        print(f\"\\n\\n--------------- {MODE} - {model} ---------------\")\n",
    "        pruner = optuna.pruners.MedianPruner(n_warmup_steps=2)\n",
    "        if model == 'DP':\n",
    "            study = optuna.create_study(directions=[\"maximize\", \"minimize\"]) # MCC ↑, ε ↓\n",
    "        else:\n",
    "            study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "        study.optimize(lambda trial: objective(trial, model), n_trials=30)\n",
    "\n",
    "        best_trial = study.best_trials[0]\n",
    "        best_params[model] = best_trial.params\n",
    "        best_scores[model] = best_trial.values\n",
    "        feat_importance[model] = optuna.importance.get_param_importances(study, target=lambda t: t.values[0])\n",
    "        studies[model] = study\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"Best parameters:       {best_params[model]}\")\n",
    "        print(f\"Best scores:           {best_scores[model]}\")\n",
    "        print(f\"Parameters importance: {feat_importance[model]}\")\n",
    "        if model == 'DP':\n",
    "            print(f\"Epsilon (ε): {best_trial.user_attrs['epsilon']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a9e68",
   "metadata": {},
   "source": [
    "## **ATTACK DETECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'binary'\n",
    "CLASSES = 2 if MODE == 'binary' else 33 # 33 types of attacks\n",
    "\n",
    "DP_mode = 'AD'\n",
    "\n",
    "best_params, feat_importance, best_scores, studies = {}, {}, {}, {}\n",
    "hyperparameter_study(best_params, feat_importance, best_scores, studies, MODE, CLASSES, DP_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e3830",
   "metadata": {},
   "source": [
    "## **ATTACK CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcfd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'multiclass'\n",
    "CLASSES = 2 if MODE == 'binary' else 33 # 33 types of attacks\n",
    "\n",
    "DP_mode = 'AC'\n",
    "\n",
    "best_params, feat_importance, best_scores, studies = {}, {}, {}, {}\n",
    "hyperparameter_study(best_params, feat_importance, best_scores, studies, MODE, CLASSES, DP_mode)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6919881,
     "sourceId": 11173096,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4885.786656,
   "end_time": "2025-04-04T10:00:22.481176",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-04T08:38:56.694520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
