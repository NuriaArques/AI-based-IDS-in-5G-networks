{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c54847",
   "metadata": {
    "papermill": {
     "duration": 0.006699,
     "end_time": "2025-04-04T08:39:03.192566",
     "exception": false,
     "start_time": "2025-04-04T08:39:03.185867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ce92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:03.206688Z",
     "iopub.status.busy": "2025-04-04T08:39:03.206466Z",
     "iopub.status.idle": "2025-04-04T08:39:05.743437Z",
     "shell.execute_reply": "2025-04-04T08:39:05.742549Z"
    },
    "papermill": {
     "duration": 2.545803,
     "end_time": "2025-04-04T08:39:05.745100",
     "exception": false,
     "start_time": "2025-04-04T08:39:03.199297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebda2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:05.760466Z",
     "iopub.status.busy": "2025-04-04T08:39:05.759939Z",
     "iopub.status.idle": "2025-04-04T08:39:05.768679Z",
     "shell.execute_reply": "2025-04-04T08:39:05.767858Z"
    },
    "papermill": {
     "duration": 0.017608,
     "end_time": "2025-04-04T08:39:05.769993",
     "exception": false,
     "start_time": "2025-04-04T08:39:05.752385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    # DDoS\n",
    "    'DDoS-ACK_Fragmentation': 'DDoS',\n",
    "    'DDoS-UDP_Flood': 'DDoS',\n",
    "    'DDoS-SlowLoris': 'DDoS',\n",
    "    'DDoS-ICMP_Flood': 'DDoS',\n",
    "    'DDoS-RSTFINFlood': 'DDoS',\n",
    "    'DDoS-PSHACK_Flood': 'DDoS',\n",
    "    'DDoS-HTTP_Flood': 'DDoS',\n",
    "    'DDoS-UDP_Fragmentation': 'DDoS',\n",
    "    'DDoS-TCP_Flood': 'DDoS',\n",
    "    'DDoS-SYN_Flood': 'DDoS',\n",
    "    'DDoS-SynonymousIP_Flood': 'DDoS',\n",
    "    'DDoS-ICMP_Fragmentation': 'DDoS',\n",
    "    \n",
    "    # DoS\n",
    "    'DoS-TCP_Flood': 'DoS',\n",
    "    'DoS-HTTP_Flood': 'DoS',\n",
    "    'DoS-SYN_Flood': 'DoS',\n",
    "    'DoS-UDP_Flood': 'DoS',\n",
    "    \n",
    "    # Brute Force\n",
    "    'DictionaryBruteForce': 'Brute Force',\n",
    "    \n",
    "    # Spoofing\n",
    "    'MITM-ArpSpoofing': 'Spoofing',\n",
    "    'DNS_Spoofing': 'Spoofing',\n",
    "    \n",
    "    # Recon\n",
    "    'Recon-PingSweep': 'Recon',\n",
    "    'Recon-OSScan': 'Recon',\n",
    "    'VulnerabilityScan': 'Recon',\n",
    "    'Recon-PortScan': 'Recon',\n",
    "    'Recon-HostDiscovery': 'Recon',\n",
    "    \n",
    "    # Web-based\n",
    "    'SqlInjection': 'Web-based',\n",
    "    'CommandInjection': 'Web-based',\n",
    "    'Backdoor_Malware': 'Web-based',\n",
    "    'Uploading_Attack': 'Web-based',\n",
    "    'XSS': 'Web-based',\n",
    "    'BrowserHijacking': 'Web-based',\n",
    "    \n",
    "    # Mirai\n",
    "    'Mirai-greip_flood': 'Mirai',\n",
    "    'Mirai-greeth_flood': 'Mirai',\n",
    "    'Mirai-udpplain': 'Mirai',\n",
    "    \n",
    "    # Benign Traffic\n",
    "    'BenignTraffic': 'Benign'\n",
    "}\n",
    "\n",
    "def GroupAttacks(label):\n",
    "    return label_mapping.get(label, 'Unknown') # Default to 'unknown'\n",
    "\n",
    "def preprocessing(df):\n",
    "    \"\"\"Initial data preprocessing\"\"\"\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    df['magnitude'] = df['magnitue']\n",
    "    df = df.drop(['magnitue'], axis=1)\n",
    "\n",
    "    # Organize label columns\n",
    "    #     label           2 categories (binary): 0: BenignTraffic, 1: Attack\n",
    "    #     attack_cat      8 categories: Bening, DDoS, DoS, Recon, Mirai, Web-based, Spoofing, Brute Force\n",
    "    #     attack_type     34 categories: Benign and 33 attacks\n",
    "    df['grouped_label'] = df['label'].map(GroupAttacks)\n",
    "    df.rename(columns={'label': 'attack_type', 'grouped_label': 'attack_cat'}, inplace=True)\n",
    "    df.replace({'attack_cat': {'BenignTraffic': 'Benign'}, 'attack_type': {'BenignTraffic': 'Benign'}}, inplace=True)\n",
    "    df['label'] = df['attack_cat'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "    # Convert categorical labels to numerical labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"attack_cat\"] = label_encoder.fit_transform(df[\"attack_cat\"])\n",
    "    \n",
    "    unique_attacks = ['Benign'] + sorted(set(df[\"attack_type\"]) - {\"Benign\"})\n",
    "    attack_type_mapping = {attack: idx for idx, attack in enumerate(unique_attacks)}\n",
    "    df[\"attack_type\"] = df[\"attack_type\"].map(attack_type_mapping)\n",
    "\n",
    "    # Feature Selection\n",
    "    less_important_features = ['drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'http', 'dns', 'telnet', 'smtp', 'ssh', 'irc', 'tcp', 'udp', 'dhcp', 'arp', 'icmp', 'ipv', 'llc', 'number']\n",
    "    df = df.drop(columns=less_important_features)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3cb74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:05.847276Z",
     "iopub.status.busy": "2025-04-04T08:39:05.847076Z",
     "iopub.status.idle": "2025-04-04T08:39:05.867868Z",
     "shell.execute_reply": "2025-04-04T08:39:05.867209Z"
    },
    "papermill": {
     "duration": 0.029303,
     "end_time": "2025-04-04T08:39:05.869107",
     "exception": false,
     "start_time": "2025-04-04T08:39:05.839804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(\"..\\\\datasets\\\\df_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778527aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:07.080698Z",
     "iopub.status.busy": "2025-04-04T08:39:07.080430Z",
     "iopub.status.idle": "2025-04-04T08:39:07.089389Z",
     "shell.execute_reply": "2025-04-04T08:39:07.088390Z"
    },
    "papermill": {
     "duration": 0.022723,
     "end_time": "2025-04-04T08:39:07.090735",
     "exception": false,
     "start_time": "2025-04-04T08:39:07.068012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = df_labels.groupby('attack_cat_name')['count'].sum().reset_index().sort_values('count', ascending=False)\n",
    "\n",
    "total = samples['count'].values.sum()\n",
    "benign = samples[samples['attack_cat_name'] == 'Benign']['count'].values[0]\n",
    "attacks = (samples[samples['attack_cat_name'] != 'Benign']['count'].values).sum()\n",
    "\n",
    "print(f\"There are {benign} ({benign/total *100:.2f}%) benign samples and {attacks} ({attacks/total*100:.2f}%) attacks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbc566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:07.144523Z",
     "iopub.status.busy": "2025-04-04T08:39:07.144281Z",
     "iopub.status.idle": "2025-04-04T08:39:07.149188Z",
     "shell.execute_reply": "2025-04-04T08:39:07.148544Z"
    },
    "papermill": {
     "duration": 0.018468,
     "end_time": "2025-04-04T08:39:07.150315",
     "exception": false,
     "start_time": "2025-04-04T08:39:07.131847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "\n",
    "def ScaleData_train(df):\n",
    "    \"\"\" Normalize training data features to ensure each has Mean = 0 and Standard Deviation = 1\"\"\"\n",
    "    \n",
    "    df_scaled = scale.fit_transform(df)\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns) \n",
    "\n",
    "def ScaleData_test(df):\n",
    "    \"\"\" Normalize validation and test data features to ensure each has Mean = 0 and Standard Deviation = 1\"\"\"\n",
    "    \n",
    "    df_scaled = scale.transform(df)\n",
    "    return pd.DataFrame(df_scaled, columns=df.columns) \n",
    "    \n",
    "def Split(df, training):\n",
    "    \"\"\"Divide DataFrame 'df' in features and labels\"\"\"\n",
    "    \n",
    "    X = df.drop(columns=['label', 'attack_cat', 'attack_type'])\n",
    "    X_scaled = ScaleData_train(X) if training else ScaleData_test(X)\n",
    "\n",
    "    # Discard 'atack_cat' as the label will not be used in any model\n",
    "    y_label = df['label']\n",
    "    y_attack = df['attack_type']\n",
    "    \n",
    "    return X_scaled, y_label, y_attack\n",
    "\n",
    "\n",
    "def to_tensor(X, y):\n",
    "    return TensorDataset(torch.tensor(X.to_numpy(), dtype=torch.float32), torch.tensor(y.to_numpy(), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6da7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:07.174096Z",
     "iopub.status.busy": "2025-04-04T08:39:07.173891Z",
     "iopub.status.idle": "2025-04-04T08:39:27.625429Z",
     "shell.execute_reply": "2025-04-04T08:39:27.624588Z"
    },
    "papermill": {
     "duration": 20.465622,
     "end_time": "2025-04-04T08:39:27.627213",
     "exception": false,
     "start_time": "2025-04-04T08:39:07.161591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = 3 # Number of nodes defined\n",
    "\n",
    "uniform_partitions = {}\n",
    "uniform_train_sets = {}\n",
    "uniform_val_sets = {}\n",
    "\n",
    "node_data = {}\n",
    "\n",
    "for i in range(nodes):\n",
    "    uniform_partitions[i] = preprocessing(pd.read_csv(f\"..\\\\datasets\\\\UniformPartitions\\\\uniform_part{i}.csv\"))\n",
    "    \n",
    "    # Split into Train (70%) and Validation (30%) \n",
    "    uniform_train_sets[i], uniform_val_sets[i] = train_test_split(uniform_partitions[i], test_size = 0.3, shuffle=True, random_state=42)\n",
    "    node_data[i] = Split(uniform_train_sets[i], True), Split(uniform_val_sets[i], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4741442",
   "metadata": {
    "papermill": {
     "duration": 0.01152,
     "end_time": "2025-04-04T08:39:27.651754",
     "exception": false,
     "start_time": "2025-04-04T08:39:27.640234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Structure of the Dictionary `node_data`:\n",
    "\n",
    " * `node_data[i][0]`: returns 3 datasets (`X_train`, `y_train_bin` and `y_train_multi`) of node `i` that will be used for training\n",
    " * `node_data[i][1]`: returns 3 datasets (`X_val`, `y_val_bin` and `y_val_multi`) of node `i` that will be used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a273c5a",
   "metadata": {
    "papermill": {
     "duration": 0.011328,
     "end_time": "2025-04-04T08:39:27.730074",
     "exception": false,
     "start_time": "2025-04-04T08:39:27.718746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ef61e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:27.753961Z",
     "iopub.status.busy": "2025-04-04T08:39:27.753689Z",
     "iopub.status.idle": "2025-04-04T08:39:32.260095Z",
     "shell.execute_reply": "2025-04-04T08:39:32.259394Z"
    },
    "papermill": {
     "duration": 4.520131,
     "end_time": "2025-04-04T08:39:32.261742",
     "exception": false,
     "start_time": "2025-04-04T08:39:27.741611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, matthews_corrcoef\n",
    "from collections import defaultdict\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import optuna\n",
    "from plotly.io import show\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f855a",
   "metadata": {
    "papermill": {
     "duration": 0.01128,
     "end_time": "2025-04-04T08:39:32.285132",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.273852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **DEEP LEARNING (DL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fca2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.308617Z",
     "iopub.status.busy": "2025-04-04T08:39:32.308395Z",
     "iopub.status.idle": "2025-04-04T08:39:32.313062Z",
     "shell.execute_reply": "2025-04-04T08:39:32.312435Z"
    },
    "papermill": {
     "duration": 0.017639,
     "end_time": "2025-04-04T08:39:32.314196",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.296557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, hidden_units, activation, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(activation())  # Activation function\n",
    "            in_features = hidden_units\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35833fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.338409Z",
     "iopub.status.busy": "2025-04-04T08:39:32.338190Z",
     "iopub.status.idle": "2025-04-04T08:39:32.343451Z",
     "shell.execute_reply": "2025-04-04T08:39:32.342801Z"
    },
    "papermill": {
     "duration": 0.018261,
     "end_time": "2025-04-04T08:39:32.344675",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.326414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, fc_units, dropout, num_conv_layers, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        # self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1 if i == 0 else num_filters, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "            for i in range(num_conv_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * input_dim, fc_units)\n",
    "        self.output = nn.Linear(fc_units, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "        # x = torch.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(torch.relu(self.fc(x)))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ade7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.374595Z",
     "iopub.status.busy": "2025-04-04T08:39:32.374225Z",
     "iopub.status.idle": "2025-04-04T08:39:32.383078Z",
     "shell.execute_reply": "2025-04-04T08:39:32.382109Z"
    },
    "papermill": {
     "duration": 0.02426,
     "end_time": "2025-04-04T08:39:32.384727",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.360467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, lstm_hidden, num_layers, fc_units, dropout, num_conv_layers, output_dim):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        # self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=1 if i == 0 else num_filters, out_channels=num_filters, kernel_size=3, padding=1)\n",
    "            for i in range(num_conv_layers)\n",
    "        ])\n",
    "        self.lstm = nn.LSTM(input_size=num_filters, hidden_size=lstm_hidden, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden, fc_units)\n",
    "        self.output = nn.Linear(fc_units, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x))\n",
    "        # x = torch.relu(self.conv1(x))\n",
    "        x = x.permute(0, 2, 1)  # Reshape for LSTM (batch, seq_len, features)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(torch.relu(self.fc(x[:, -1, :])))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26f73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.421903Z",
     "iopub.status.busy": "2025-04-04T08:39:32.421669Z",
     "iopub.status.idle": "2025-04-04T08:39:32.427772Z",
     "shell.execute_reply": "2025-04-04T08:39:32.427153Z"
    },
    "papermill": {
     "duration": 0.025271,
     "end_time": "2025-04-04T08:39:32.428914",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.403643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pytorch(model, train_loader, val_loader, optimizer, criterion, epochs, threshold=0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    # correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # if threshold != 0 :\n",
    "            #     probs = F.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "            #     max_probs, predicted = torch.max(probs, dim=1)  # Get highest prob & predicted class\n",
    "                \n",
    "            #     # Multiclass classification: assign 'Unknown' class if confidence is below the threshold\n",
    "            #     predicted[max_probs < threshold] = 0  # Label `0` for 'Unknown'\n",
    "\n",
    "            # else:\n",
    "            #     _, predicted = torch.max(outputs, 1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted.cpu())\n",
    "            all_labels.append(y_batch.cpu())\n",
    "    y_pred, y_true = torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "    # if threshold != 0:\n",
    "    #     # Compute accuracy (excluding 'Unknown' cases)\n",
    "    #     valid_mask = y_pred != 0  # Ignore 'Unknown' class\n",
    "    #     accuracy = (y_pred[valid_mask] == y_true[valid_mask]).float().mean().item() if valid_mask.any() else 0.0\n",
    "    #     print(f\"ValAcc: {accuracy}\")\n",
    "\n",
    "    # val_acc = correct / total\n",
    "    Accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {(Accuracy*100):.2f}%\")\n",
    "    mcc = evaluate(model, [], y_true, y_pred, 'pytorch')\n",
    "    return model.state_dict(), mcc  # Return local trained model weights and evaluation metrics\n",
    "    \n",
    "    \n",
    "\n",
    "# def test_pytorch(model, X_test, y_test):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32).to(device)\n",
    "#     y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long).to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(X_test_tensor)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         accuracy = (predicted == y_test_tensor).float().mean().item()\n",
    "\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88834a2",
   "metadata": {
    "papermill": {
     "duration": 0.011402,
     "end_time": "2025-04-04T08:39:32.451750",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.440348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **MACHINE LEARNING (ML)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a98d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.475504Z",
     "iopub.status.busy": "2025-04-04T08:39:32.475187Z",
     "iopub.status.idle": "2025-04-04T08:39:32.480145Z",
     "shell.execute_reply": "2025-04-04T08:39:32.479488Z"
    },
    "papermill": {
     "duration": 0.018309,
     "end_time": "2025-04-04T08:39:32.481469",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.463160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_tree_model(model_class, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train a tree-based model on a given client's data\"\"\"\n",
    "    \n",
    "    model = model_class\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    return model\n",
    "\n",
    "\n",
    "def aggregate_tree_models(model_class, models, X_val, y_val, X_train, mode):\n",
    "\n",
    "    # Simple aggregation: choose the best model based on validation accuracy\n",
    "    if mode == \"Avg\":\n",
    "        best_model = max(models, key=lambda m: m.score(X_val, y_val))\n",
    "    \n",
    "    # Train a global XGBoost model using averaged soft predictions from clients\n",
    "    else:\n",
    "        soft_predictions = np.mean([model.predict_proba(X_train) for model in models], axis=0)\n",
    "        pseudo_labels = np.argmax(soft_predictions, axis=1)  # Convert to class labels\n",
    "    \n",
    "        # Train new XGBoost model on pseudo-labels\n",
    "        best_model = model_class\n",
    "        best_model.fit(X_train, pseudo_labels)\n",
    "            \n",
    "    return best_model\n",
    "\n",
    "\n",
    "def test_tree_model(model, X_test, y_test):\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa7d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.527516Z",
     "iopub.status.busy": "2025-04-04T08:39:32.527296Z",
     "iopub.status.idle": "2025-04-04T08:39:32.530646Z",
     "shell.execute_reply": "2025-04-04T08:39:32.530002Z"
    },
    "papermill": {
     "duration": 0.016401,
     "end_time": "2025-04-04T08:39:32.531819",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.515418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 3  # Number of federated clients\n",
    "HYPERPARAM_TUNING = True  # Set to False to disable Optuna tuning\n",
    "EPOCHS = 5  # Training epochs for PyTorch models\n",
    "input_dim = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bbc35",
   "metadata": {
    "papermill": {
     "duration": 0.011097,
     "end_time": "2025-04-04T08:39:32.589059",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.577962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **HYPERPARAMETER TUNNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04cbe98",
   "metadata": {},
   "source": [
    "## **STUDIES DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af02f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.612482Z",
     "iopub.status.busy": "2025-04-04T08:39:32.612253Z",
     "iopub.status.idle": "2025-04-04T08:39:32.623249Z",
     "shell.execute_reply": "2025-04-04T08:39:32.622651Z"
    },
    "papermill": {
     "duration": 0.023973,
     "end_time": "2025-04-04T08:39:32.624459",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.600486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "activation_dict = {\n",
    "    \"ReLU\": torch.nn.ReLU,\n",
    "    \"LeakyReLU\": torch.nn.LeakyReLU,\n",
    "    \"Tanh\": torch.nn.Tanh\n",
    "}\n",
    "\n",
    "def define_optimizer(model, optimizer_name, lr):\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr) \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def tune_DNN(trial, X_train, y_train, X_val, y_val, MODE):\n",
    "\n",
    "    hidden_layers = trial.suggest_int(\"hidden_layers\", 1, 5)\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 16, 128)\n",
    "    activation_name = trial.suggest_categorical(\"activation\", list(activation_dict.keys()))\n",
    "    activation_fn = activation_dict[activation_name]\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "\n",
    "    model = DNN(input_dim, hidden_layers, hidden_units, activation_fns, s, output_dim=CLASSES).to(device)\n",
    "    criterion = nn.CrossEntropyLoss() if MODE == 'binary' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, optimizer_name, lr)\n",
    "    \n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**batch_size, shuffle=False)\n",
    "    \n",
    "    _, mcc = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 10) # Small number of epochs for tuning\n",
    "    return mcc\n",
    "    \n",
    "\n",
    "def tune_CNN(trial, X_train, y_train, X_val, y_val, MODE):\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 32, 128)\n",
    "    fc_units = trial.suggest_int(\"fc_units\", 16, 64)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "    \n",
    "    model = CNN(input_dim, num_filters=num_filters, fc_units=fc_units, dropout=dropout, num_conv_layers=num_conv_layers, output_dim=CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss() if MODE == 'binary' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, optimizer_name, lr)\n",
    "\n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**batch_size, shuffle=False)\n",
    "    \n",
    "    _, mcc = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 10) # Small number of epochs for tuning\n",
    "    return mcc\n",
    "    \n",
    "    \n",
    "def tune_CNN_LSTM(trial, X_train, y_train, X_val, y_val, MODE):\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 32, 128)\n",
    "    lstm_hidden = trial.suggest_int(\"lstm_hidden\", 16, 64)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    fc_units = trial.suggest_int(\"fc_units\", 16, 64)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "    \n",
    "    model = CNN_LSTM(input_dim, num_filters=num_filters, lstm_hidden=lstm_hidden, num_layers=num_layers, fc_units=fc_units, dropout=dropout, num_conv_layers=num_conv_layerss, output_dim=CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss() if MODE == 'binary' else nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = define_optimizer(model, optimizer_name, lr)\n",
    "    \n",
    "    train_loader = DataLoader(to_tensor(X_train, y_train), batch_size=2**batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(to_tensor(X_val, y_val), batch_size=2**batch_size, shuffle=False)\n",
    "    \n",
    "    _, mcc = train_pytorch(model, train_loader, val_loader, optimizer, criterion, 10) # Small number of epochs for tuning\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f1580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.682425Z",
     "iopub.status.busy": "2025-04-04T08:39:32.682144Z",
     "iopub.status.idle": "2025-04-04T08:39:32.687632Z",
     "shell.execute_reply": "2025-04-04T08:39:32.687011Z"
    },
    "papermill": {
     "duration": 0.019251,
     "end_time": "2025-04-04T08:39:32.688844",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.669593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tune_XGB(trial, X_train, y_train, X_val, y_val):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3), # learning rate\n",
    "        'reg_alpha': trial.suggest_int('reg_alpha', 0, 100, step=5), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_int('reg_lambda', 0, 100, step=5) # L2 regularization\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(objective = 'binary:logistic', random_state=42, **params) #  eval_metric = 'auc'\n",
    "    model.fit(X_train, y_train)\n",
    "    return evaluate(model, X_val, y_val, [], 'tree')\n",
    "    \n",
    "\n",
    "def tune_LGBM(trial, X_train, y_train, X_val, y_val):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        'reg_alpha': trial.suggest_int('reg_alpha', 0, 100, step=5), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_int('reg_lambda', 0, 100, step=5) # L2 regularization\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier(objective = 'binary', random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return evaluate(model, X_val, y_val, [], 'tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431e810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.651079Z",
     "iopub.status.busy": "2025-04-04T08:39:32.650839Z",
     "iopub.status.idle": "2025-04-04T08:39:32.654860Z",
     "shell.execute_reply": "2025-04-04T08:39:32.653828Z"
    },
    "papermill": {
     "duration": 0.020492,
     "end_time": "2025-04-04T08:39:32.656418",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.635926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X, y_true, y_pred, model_type):\n",
    "    if model_type =='tree':\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "    # accuracy = accuracy_score(y_true, y_pred)\n",
    "    # precision = precision_score(y_true, y_pred)\n",
    "    # recall = recall_score(y_true, y_pred)\n",
    "    # f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc5889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.713307Z",
     "iopub.status.busy": "2025-04-04T08:39:32.713111Z",
     "iopub.status.idle": "2025-04-04T08:39:32.717688Z",
     "shell.execute_reply": "2025-04-04T08:39:32.717135Z"
    },
    "papermill": {
     "duration": 0.017791,
     "end_time": "2025-04-04T08:39:32.718797",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.701006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def full_dataset(MODE):\n",
    "    j = 1 if (MODE == 'binary') else 2\n",
    "    \n",
    "    X_train = pd.concat([node_data[i][0][0] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "    y_train = pd.concat([node_data[i][0][j] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "    \n",
    "    X_val = pd.concat([node_data[i][1][0] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "    y_val = pd.concat([node_data[i][1][j] for i in range(NUM_CLIENTS)], ignore_index=True)\n",
    "\n",
    "    # Only exclude label 0 ('Benign') if MODE is not 'binary'\n",
    "    if MODE != 'binary':\n",
    "        mask_train = y_train != 0\n",
    "        mask_val = y_val != 0\n",
    "\n",
    "        X_train, y_train = X_train[mask_train], y_train[mask_train]\n",
    "        X_val, y_val = X_val[mask_val], y_val[mask_val]\n",
    "\n",
    "        # Ensure labels are zero-indexed (1-33 -> 0-32)\n",
    "        y_train = y_train - 1\n",
    "        y_val = y_val - 1\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a9e68",
   "metadata": {},
   "source": [
    "## **ATTACK DETECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'binary'\n",
    "CLASSES = 2 if MODE == 'binary' else 33 # 33 types of attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ed670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T08:39:32.742700Z",
     "iopub.status.busy": "2025-04-04T08:39:32.742475Z",
     "iopub.status.idle": "2025-04-04T10:00:18.984491Z",
     "shell.execute_reply": "2025-04-04T10:00:18.983673Z"
    },
    "papermill": {
     "duration": 4846.255393,
     "end_time": "2025-04-04T10:00:18.985901",
     "exception": false,
     "start_time": "2025-04-04T08:39:32.730508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params, feat_importance, best_scores, studies = {}, {}, {}, {}\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    return models_dict[model_name](trial)\n",
    "\n",
    "X_train, y_train, X_val, y_val = full_dataset(MODE)\n",
    "\n",
    "models_dict = {\n",
    "    'DNN': lambda trial: tune_DNN(trial, X_train, y_train, X_val, y_val, MODE),\n",
    "    'CNN': lambda trial: tune_CNN(trial, X_train, y_train, X_val, y_val, MODE),\n",
    "    'CNN-LSTM': lambda trial: tune_CNN_LSTM(trial, X_train, y_train, X_val, y_val, MODE),\n",
    "    'XGB': lambda trial: tune_XGB(trial, X_train, y_train, X_val, y_val),\n",
    "    'LGBM': lambda trial: tune_LGBM(trial, X_train, y_train, X_val, y_val)\n",
    "}\n",
    "\n",
    "\n",
    "for model in models_dict.keys():\n",
    "    \n",
    "    print(f\"\\n\\n--------------- {model} ---------------\")\n",
    "    study = optuna.create_study(directions=[\"maximize\"])\n",
    "    study.optimize(lambda trial: objective(trial, model), n_trials=30)\n",
    "\n",
    "    best_trial = study.best_trials[0]\n",
    "    best_params[model] = best_trial.params\n",
    "    best_scores[model] = best_trial.values\n",
    "    feat_importance[model] = optuna.importance.get_param_importances(study, target=lambda t: t.values[0])\n",
    "    studies[model] = study\n",
    "    \n",
    "    print(f\"    Best parameters:       {best_params[model]}\")\n",
    "    print(f\"    Best scores:           {best_scores[model]}\")\n",
    "    print(f\"    Parameters importance: {feat_importance[model]}\")\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130427ba",
   "metadata": {
    "papermill": {
     "duration": 0.015616,
     "end_time": "2025-04-04T10:00:19.018731",
     "exception": false,
     "start_time": "2025-04-04T10:00:19.003115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961a3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T10:00:19.049709Z",
     "iopub.status.busy": "2025-04-04T10:00:19.049184Z",
     "iopub.status.idle": "2025-04-04T10:00:19.054005Z",
     "shell.execute_reply": "2025-04-04T10:00:19.053014Z"
    },
    "papermill": {
     "duration": 0.021141,
     "end_time": "2025-04-04T10:00:19.055207",
     "exception": false,
     "start_time": "2025-04-04T10:00:19.034066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model, values in best_params.items():\n",
    "    print(f\"\\n\\n\\n ========================== AD: {model} ==========================\")\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters: {values}\")\n",
    "    print(f\"Best MCC : {best_scores[model]}\") \n",
    "    print(f\"\\nHyperparameters importance: {feat_importance[model]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e3830",
   "metadata": {},
   "source": [
    "## **ATTACK CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcfd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'multiclass'\n",
    "CLASSES = 2 if MODE == 'binary' else 33 # 33 types of attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1834990",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, feat_importance, best_scores, studies = {}, {}, {}, {}\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    return models_dict[model_name](trial)\n",
    "\n",
    "X_train, y_train, X_val, y_val = full_dataset(MODE)\n",
    "\n",
    "models_dict = {\n",
    "    'DNN': lambda trial: tune_DNN(trial, X_train, y_train, X_val, y_val, MODE),\n",
    "    'CNN': lambda trial: tune_CNN(trial, X_train, y_train, X_val, y_val, MODE),\n",
    "    'CNN-LSTM': lambda trial: tune_CNN_LSTM(trial, X_train, y_train, X_val, y_val, MODE),\n",
    "    'XGB': lambda trial: tune_XGB(trial, X_train, y_train, X_val, y_val),\n",
    "    'LGBM': lambda trial: tune_LGBM(trial, X_train, y_train, X_val, y_val)\n",
    "}\n",
    "\n",
    "\n",
    "for model in models_dict.keys():\n",
    "    \n",
    "    print(f\"\\n\\n--------------- {model} ---------------\")\n",
    "    study = optuna.create_study(directions=[\"maximize\"])\n",
    "    study.optimize(lambda trial: objective(trial, model), n_trials=30)\n",
    "\n",
    "    best_trial = study.best_trials[0]\n",
    "    best_params[model] = best_trial.params\n",
    "    best_scores[model] = best_trial.values\n",
    "    feat_importance[model] = optuna.importance.get_param_importances(study, target=lambda t: t.values[0])\n",
    "    studies[model] = study\n",
    "    \n",
    "    print(f\"    Best parameters:       {best_params[model]}\")\n",
    "    print(f\"    Best scores:           {best_scores[model]}\")\n",
    "    print(f\"    Parameters importance: {feat_importance[model]}\")\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b94a6",
   "metadata": {},
   "source": [
    "### **RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, values in best_params.items():\n",
    "    print(f\"\\n\\n\\n ========================== AC: {model} ==========================\")\n",
    "    \n",
    "    print(f\"\\nBest hyperparameters: {values}\")\n",
    "    print(f\"Best MCC : {best_scores[model]}\") \n",
    "    print(f\"\\nHyperparameters importance: {feat_importance[model]}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6919881,
     "sourceId": 11173096,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4885.786656,
   "end_time": "2025-04-04T10:00:22.481176",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-04T08:38:56.694520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
